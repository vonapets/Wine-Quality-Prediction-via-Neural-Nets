# -*- coding: utf-8 -*-
"""Assignment 3 Vlad Stepanov GB657.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PBkIiLN4oUNZJRri5P5PLIbP2-T02Taz
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

from mpl_toolkits.mplot3d import Axes3D
from sklearn.decomposition import PCA


from sklearn.preprocessing import scale
from sklearn.metrics import mean_squared_error, roc_curve, auc, r2_score
import statsmodels.formula.api as smf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score


import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.callbacks import EarlyStopping

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader

import time

df= pd.read_csv('/content/Assignment_1_winequality (2).csv')

df.head()

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")
if device.type == 'cuda':
    print(f"GPU: {torch.cuda.get_device_name(0)}")

chem_cols = [c for c in df.columns if c != 'quality']
X = df[chem_cols]
y = df['quality']

torch.manual_seed(42)
np.random.seed(42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)
y_train_tensor = torch.LongTensor(y_train.values).to(device)
X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)
y_test_tensor = torch.LongTensor(y_test.values).to(device)

def sigmoid(x):
    return(1 / (1 + np.exp(-x)))

# After standardizing your data, add this:
def visualize_data_3d(X, y, title="Wine Data 3D Visualization"):
    """Create 3D scatter plot of wine data using PCA."""
    # Reduce to 3 dimensions
    pca = PCA(n_components=3)
    X_3d = pca.fit_transform(X)

    fig = plt.figure(figsize=(12, 9))
    ax = fig.add_subplot(111, projection='3d')

    # Color by quality (0 or 1)
    colors = ['gray' if label == 0 else 'black' for label in y]

    ax.scatter(X_3d[:, 0], X_3d[:, 1], X_3d[:, 2],
               c=colors, alpha=0.6, s=50)

    ax.set_xlabel('x1_Label', fontsize=12)
    ax.set_ylabel('x2_Label', fontsize=12)
    ax.set_zlabel('y_Label', fontsize=12)
    ax.set_title(title, fontsize=14, fontweight='bold')

    plt.show()

# Use it like this:
visualize_data_3d(X_train_scaled, y_train, "Training Set - 3D PCA")
visualize_data_3d(X_test_scaled, y_test, "Test Set - 3D PCA")

"""**Image 1: Training Set - 3D PCA:**
Shows approximately 1,200 training samples reduced from 11 features to 3 dimensions. The gray (low quality) and black (high quality) wine classes overlap significantly in the center, explaining why neural networks achieve ~87-90% accuracy rather than near-perfect scores. The substantial overlap indicates that some wines with similar chemical profiles have different quality ratings, making perfect classification impossible.


**Image 2: Test Set - 3D PCA:**
Shows approximately 400 test samples in the same 3D space. The distribution closely matches the training set with similar class overlap and spread, confirming the 75/25 split created representative data. This similarity validates that the neural networks are being fairly evaluated on data from the same distribution they were trained on, making the ~88-90% test accuracy reliable.

**Key Insight:**
The class overlap in both plots explains why even the best models can't achieve perfect accuracy - the problem has inherent ambiguity where similar chemical compositions yield different quality ratings.
"""

def build_model(learning_rate=0.001, activation='relu', num_layers=2, num_classes=2):
    """Build TensorFlow/Keras model."""
    model = keras.Sequential()

    if num_layers == 2:
        model.add(layers.Dense(64, activation=activation, input_shape=(11,)))
        model.add(layers.Dense(32, activation=activation))
        model.add(layers.Dense(num_classes, activation='softmax'))
    elif num_layers == 4:
        model.add(layers.Dense(64, activation=activation, input_shape=(11,)))
        model.add(layers.Dense(32, activation=activation))
        model.add(layers.Dense(16, activation=activation))
        model.add(layers.Dense(8, activation=activation))
        model.add(layers.Dense(num_classes, activation='softmax'))

    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer,
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    return model


class EpochMetricsCallback(keras.callbacks.Callback):
    """Custom callback to track metrics per epoch."""
    def __init__(self, X_test, y_test):
        super().__init__()
        self.X_test = X_test
        self.y_test = y_test
        self.history_data = {
            'epoch': [],
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': [],
            'test_acc': [],
            'test_auc': []
        }

    def on_epoch_end(self, epoch, logs=None):
        # Get predictions on test set
        y_pred_proba = self.model.predict(self.X_test, verbose=0)
        y_score = y_pred_proba[:, 1]  # Probability of class 1
        y_pred = np.argmax(y_pred_proba, axis=1)

        test_acc = accuracy_score(self.y_test, y_pred)
        test_auc = roc_auc_score(self.y_test, y_score)

        # Store metrics
        self.history_data['epoch'].append(epoch + 1)
        self.history_data['train_loss'].append(logs.get('loss'))
        self.history_data['train_acc'].append(logs.get('accuracy'))
        self.history_data['val_loss'].append(logs.get('val_loss'))
        self.history_data['val_acc'].append(logs.get('val_accuracy'))
        self.history_data['test_acc'].append(test_acc)
        self.history_data['test_auc'].append(test_auc)


def train_and_evaluate_tensorflow(learning_rate=0.001, activation='relu', num_layers=2,
                                   batch_size=32, epochs=50, early_stopping=False,
                                   experiment_name=""):
    """Train TensorFlow model with epoch-by-epoch tracking."""

    num_classes = len(np.unique(y_train))
    model = build_model(learning_rate=learning_rate, activation=activation,
                        num_layers=num_layers, num_classes=num_classes)

    # Split training data for validation (80-20 split of training set)
    X_train_sub, X_val, y_train_sub, y_val = train_test_split(
        X_train_scaled, y_train, test_size=0.2, random_state=42
    )

    # Setup callbacks
    epoch_metrics = EpochMetricsCallback(X_test_scaled, y_test)
    callbacks = [epoch_metrics]

    if early_stopping:
        early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
        callbacks.append(early_stop)

    print(f"\n{'='*80}")
    print(f"Training: {experiment_name}")
    print(f"{'='*80}")

    # Train model
    history = model.fit(
        X_train_sub, y_train_sub,
        batch_size=batch_size,
        epochs=epochs,
        validation_data=(X_val, y_val),
        callbacks=callbacks,
        verbose=0  # Suppress default output
    )

    # Print epoch metrics every 5 epochs
    print(f"{'Epoch':<8} {'Train Loss':<12} {'Val Loss':<12} {'Train Acc':<12} {'Val Acc':<12} {'Test Acc':<12} {'Test AUC':<12}")
    print(f"{'-'*80}")

    for i in range(len(epoch_metrics.history_data['epoch'])):
        if (i + 1) % 5 == 0 or (i + 1) == len(epoch_metrics.history_data['epoch']):
            print(f"{epoch_metrics.history_data['epoch'][i]:<8} "
                  f"{epoch_metrics.history_data['train_loss'][i]:<12.6f} "
                  f"{epoch_metrics.history_data['val_loss'][i]:<12.6f} "
                  f"{epoch_metrics.history_data['train_acc'][i]:<12.4f} "
                  f"{epoch_metrics.history_data['val_acc'][i]:<12.4f} "
                  f"{epoch_metrics.history_data['test_acc'][i]:<12.4f} "
                  f"{epoch_metrics.history_data['test_auc'][i]:<12.4f}")

    epochs_run = len(epoch_metrics.history_data['epoch'])

    # Final evaluation on test set
    y_pred_proba = model.predict(X_test_scaled, verbose=0)
    y_score = y_pred_proba[:, 1]
    y_pred = np.argmax(y_pred_proba, axis=1)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
    auc = roc_auc_score(y_test, y_score)

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'auc': auc,
        'epochs_run': epochs_run,
        'history': pd.DataFrame(epoch_metrics.history_data)
    }


def create_visualizations(history_df, experiment_name):
    """Create 3 visualizations for each experiment."""

    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    fig.suptitle(f'{experiment_name} - Training Progress (TensorFlow)', fontsize=16, fontweight='bold')

    # Plot 1: Training and Validation Loss
    axes[0].plot(history_df['epoch'], history_df['train_loss'], 'b-', label='Train Loss', linewidth=2)
    axes[0].plot(history_df['epoch'], history_df['val_loss'], 'r-', label='Val Loss', linewidth=2)
    axes[0].set_xlabel('Epoch', fontsize=12)
    axes[0].set_ylabel('Loss', fontsize=12)
    axes[0].set_title('Training vs Validation Loss', fontsize=14)
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    # Plot 2: Accuracy Comparison (Train, Val, Test)
    axes[1].plot(history_df['epoch'], history_df['train_acc'], 'b-', label='Train Acc', linewidth=2)
    axes[1].plot(history_df['epoch'], history_df['val_acc'], 'g-', label='Val Acc', linewidth=2)
    axes[1].plot(history_df['epoch'], history_df['test_acc'], 'r-', label='Test Acc', linewidth=2)
    axes[1].set_xlabel('Epoch', fontsize=12)
    axes[1].set_ylabel('Accuracy', fontsize=12)
    axes[1].set_title('Accuracy Across Sets', fontsize=14)
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)

    # Plot 3: Test AUC Over Time
    axes[2].plot(history_df['epoch'], history_df['test_auc'], 'purple', linewidth=2)
    axes[2].set_xlabel('Epoch', fontsize=12)
    axes[2].set_ylabel('AUC', fontsize=12)
    axes[2].set_title('Test AUC Over Training', fontsize=14)
    axes[2].grid(True, alpha=0.3)
    axes[2].axhline(y=history_df['test_auc'].max(), color='r', linestyle='--',
                    alpha=0.5, label=f'Max AUC: {history_df["test_auc"].max():.4f}')
    axes[2].legend()

    plt.tight_layout()
    plt.show()


# Run all experiments with tracking
results = []
all_histories = {}

experiments = [
    ("Control (Base Model)", {'learning_rate': 0.001, 'activation': 'relu', 'num_layers': 2, 'batch_size': 32, 'epochs': 50, 'early_stopping': False}),
    ("Q1-a: LR=0.0001 (Lower)", {'learning_rate': 0.0001, 'activation': 'relu', 'num_layers': 2, 'batch_size': 32, 'epochs': 50, 'early_stopping': False}),
    ("Q1-b: LR=0.01 (Higher)", {'learning_rate': 0.01, 'activation': 'relu', 'num_layers': 2, 'batch_size': 32, 'epochs': 50, 'early_stopping': False}),
    ("Q2: Activation=tanh", {'learning_rate': 0.001, 'activation': 'tanh', 'num_layers': 2, 'batch_size': 32, 'epochs': 50, 'early_stopping': False}),
    ("Q3: Four Layers", {'learning_rate': 0.001, 'activation': 'relu', 'num_layers': 4, 'batch_size': 32, 'epochs': 50, 'early_stopping': False}),
    ("Q4-a: Batch Size=128 (Larger)", {'learning_rate': 0.001, 'activation': 'relu', 'num_layers': 2, 'batch_size': 128, 'epochs': 50, 'early_stopping': False}),
    ("Q4-b: Batch Size=8 (Smaller)", {'learning_rate': 0.001, 'activation': 'relu', 'num_layers': 2, 'batch_size': 8, 'epochs': 50, 'early_stopping': False}),
    ("Q5: Early Stopping (Patience=5)", {'learning_rate': 0.001, 'activation': 'relu', 'num_layers': 2, 'batch_size': 32, 'epochs': 100, 'early_stopping': True})
]

for exp_name, params in experiments:
    metrics = train_and_evaluate_tensorflow(**params, experiment_name=exp_name)

    results.append({
        'Experiment': exp_name,
        'Learning Rate': params['learning_rate'],
        'Activation': params['activation'],
        'Num Layers': params['num_layers'],
        'Batch Size': params['batch_size'],
        'Early Stopping': 'Yes' if params['early_stopping'] else 'No',
        'Epochs Run': metrics['epochs_run'],
        'Accuracy': metrics['accuracy'],
        'Precision': metrics['precision'],
        'Recall': metrics['recall'],
        'F1 Score': metrics['f1'],
        'AUC': metrics['auc']
    })

    all_histories[exp_name] = metrics['history']
    create_visualizations(metrics['history'], exp_name)

# Create final results table
results_df = pd.DataFrame(results)

print("\n" + "="*100)
print("TENSORFLOW FINAL EXPERIMENT RESULTS TABLE")
print("="*100)
print(results_df.to_string(index=False))
print("="*100)

print("\nTensorFlow experiments complete!")

class WineNN(nn.Module):
    """Neural network for wine quality classification."""
    def __init__(self, activation='relu', num_layers=2, num_classes=2):
        super(WineNN, self).__init__()

        if activation == 'relu':
            act_fn = nn.ReLU()
        elif activation == 'tanh':
            act_fn = nn.Tanh()

        if num_layers == 2:
            self.layers = nn.Sequential(
                nn.Linear(11, 64),
                act_fn,
                nn.Linear(64, 32),
                act_fn,
                nn.Linear(32, num_classes)
            )
        elif num_layers == 4:
            self.layers = nn.Sequential(
                nn.Linear(11, 64),
                act_fn,
                nn.Linear(64, 32),
                act_fn,
                nn.Linear(32, 16),
                act_fn,
                nn.Linear(16, 8),
                act_fn,
                nn.Linear(8, num_classes)
            )

    def forward(self, x):
        return self.layers(x)


def train_and_evaluate_pytorch(learning_rate=0.001, activation='relu', num_layers=2,
                                batch_size=32, epochs=50, early_stopping=False,
                                experiment_name=""):
    """Train PyTorch model with epoch-by-epoch tracking."""

    num_classes = len(np.unique(y_train))
    model = WineNN(activation=activation, num_layers=num_layers, num_classes=num_classes).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    # Split training data for validation
    train_size = int(0.8 * len(X_train_tensor))
    indices = torch.randperm(len(X_train_tensor))
    train_indices = indices[:train_size]
    val_indices = indices[train_size:]

    X_train_sub = X_train_tensor[train_indices]
    y_train_sub = y_train_tensor[train_indices]
    X_val = X_train_tensor[val_indices]
    y_val = y_train_tensor[val_indices]

    train_dataset = TensorDataset(X_train_sub, y_train_sub)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    # Track metrics per epoch
    epoch_history = {
        'epoch': [],
        'train_loss': [],
        'val_loss': [],
        'train_acc': [],
        'val_acc': [],
        'test_acc': [],
        'test_auc': []
    }

    best_val_loss = float('inf')
    patience_counter = 0
    epochs_run = 0

    print(f"\n{'='*80}")
    print(f"Training: {experiment_name}")
    print(f"{'='*80}")
    print(f"{'Epoch':<8} {'Train Loss':<12} {'Val Loss':<12} {'Train Acc':<12} {'Val Acc':<12} {'Test Acc':<12} {'Test AUC':<12}")
    print(f"{'-'*80}")

    for epoch in range(epochs):
        # Training phase
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0

        for batch_X, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            train_total += batch_y.size(0)
            train_correct += (predicted == batch_y).sum().item()

        train_loss /= len(train_loader)
        train_acc = train_correct / train_total

        # Validation phase
        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val)
            val_loss = criterion(val_outputs, y_val).item()
            _, val_predicted = torch.max(val_outputs.data, 1)
            val_acc = (val_predicted == y_val).float().mean().item()

            # Test set evaluation
            test_outputs = model(X_test_tensor)
            y_pred_proba = torch.softmax(test_outputs, dim=1).cpu().numpy()
            y_score = y_pred_proba[:, 1]
            y_pred = torch.argmax(test_outputs, dim=1).cpu().numpy()

            test_acc = accuracy_score(y_test, y_pred)
            test_auc = roc_auc_score(y_test, y_score)

        # Store epoch metrics
        epoch_history['epoch'].append(epoch + 1)
        epoch_history['train_loss'].append(train_loss)
        epoch_history['val_loss'].append(val_loss)
        epoch_history['train_acc'].append(train_acc)
        epoch_history['val_acc'].append(val_acc)
        epoch_history['test_acc'].append(test_acc)
        epoch_history['test_auc'].append(test_auc)

        epochs_run = epoch + 1

        # Print every 5 epochs or if early stopping
        if (epoch + 1) % 5 == 0 or early_stopping:
            print(f"{epoch+1:<8} {train_loss:<12.6f} {val_loss:<12.6f} {train_acc:<12.4f} {val_acc:<12.4f} {test_acc:<12.4f} {test_auc:<12.4f}")

        # Early stopping check
        if early_stopping:
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                best_model_state = model.state_dict().copy()
            else:
                patience_counter += 1
                if patience_counter >= 5:
                    print(f"Early stopping at epoch {epoch+1}")
                    model.load_state_dict(best_model_state)
                    break

class WineNN(nn.Module):
    """Neural network for wine quality classification."""
    def __init__(self, activation='relu', num_layers=2, num_classes=2):
        super(WineNN, self).__init__()

        if activation == 'relu':
            act_fn = nn.ReLU()
        elif activation == 'tanh':
            act_fn = nn.Tanh()

        if num_layers == 2:
            self.layers = nn.Sequential(
                nn.Linear(11, 64),
                act_fn,
                nn.Linear(64, 32),
                act_fn,
                nn.Linear(32, num_classes)
            )
        elif num_layers == 4:
            self.layers = nn.Sequential(
                nn.Linear(11, 64),
                act_fn,
                nn.Linear(64, 32),
                act_fn,
                nn.Linear(32, 16),
                act_fn,
                nn.Linear(16, 8),
                act_fn,
                nn.Linear(8, num_classes)
            )

    def forward(self, x):
        return self.layers(x)


def train_and_evaluate_pytorch(learning_rate=0.001, activation='relu', num_layers=2,
                                batch_size=32, epochs=50, early_stopping=False,
                                experiment_name=""):
    """Train PyTorch model with epoch-by-epoch tracking."""

    num_classes = len(np.unique(y_train))
    model = WineNN(activation=activation, num_layers=num_layers, num_classes=num_classes).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    # Split training data for validation
    train_size = int(0.8 * len(X_train_tensor))
    indices = torch.randperm(len(X_train_tensor))
    train_indices = indices[:train_size]
    val_indices = indices[train_size:]

    X_train_sub = X_train_tensor[train_indices]
    y_train_sub = y_train_tensor[train_indices]
    X_val = X_train_tensor[val_indices]
    y_val = y_train_tensor[val_indices]

    train_dataset = TensorDataset(X_train_sub, y_train_sub)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    # Track metrics per epoch
    epoch_history = {
        'epoch': [],
        'train_loss': [],
        'val_loss': [],
        'train_acc': [],
        'val_acc': [],
        'test_acc': [],
        'test_auc': []
    }

    best_val_loss = float('inf')
    patience_counter = 0
    epochs_run = 0

    print(f"\n{'='*80}")
    print(f"Training: {experiment_name}")
    print(f"{'='*80}")
    print(f"{'Epoch':<8} {'Train Loss':<12} {'Val Loss':<12} {'Train Acc':<12} {'Val Acc':<12} {'Test Acc':<12} {'Test AUC':<12}")
    print(f"{'-'*80}")

    for epoch in range(epochs):
        # Training phase
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0

        for batch_X, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            train_total += batch_y.size(0)
            train_correct += (predicted == batch_y).sum().item()

        train_loss /= len(train_loader)
        train_acc = train_correct / train_total

        # Validation phase
        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val)
            val_loss = criterion(val_outputs, y_val).item()
            _, val_predicted = torch.max(val_outputs.data, 1)
            val_acc = (val_predicted == y_val).float().mean().item()

            # Test set evaluation
            test_outputs = model(X_test_tensor)
            y_pred_proba = torch.softmax(test_outputs, dim=1).cpu().numpy()
            y_score = y_pred_proba[:, 1]
            y_pred = torch.argmax(test_outputs, dim=1).cpu().numpy()

            test_acc = accuracy_score(y_test, y_pred)
            test_auc = roc_auc_score(y_test, y_score)

        # Store epoch metrics
        epoch_history['epoch'].append(epoch + 1)
        epoch_history['train_loss'].append(train_loss)
        epoch_history['val_loss'].append(val_loss)
        epoch_history['train_acc'].append(train_acc)
        epoch_history['val_acc'].append(val_acc)
        epoch_history['test_acc'].append(test_acc)
        epoch_history['test_auc'].append(test_auc)

        epochs_run = epoch + 1

        # Print every 5 epochs or if early stopping
        if (epoch + 1) % 5 == 0 or early_stopping:
            print(f"{epoch+1:<8} {train_loss:<12.6f} {val_loss:<12.6f} {train_acc:<12.4f} {val_acc:<12.4f} {test_acc:<12.4f} {test_auc:<12.4f}")

        # Early stopping check
        if early_stopping:
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                best_model_state = model.state_dict().copy()
            else:
                patience_counter += 1
                if patience_counter >= 5:
                    print(f"Early stopping at epoch {epoch+1}")
                    model.load_state_dict(best_model_state)
                    break

    # Final evaluation
    model.eval()
    with torch.no_grad():
        test_outputs = model(X_test_tensor)
        y_pred_proba = torch.softmax(test_outputs, dim=1).cpu().numpy()
        y_score = y_pred_proba[:, 1]
        y_pred = torch.argmax(test_outputs, dim=1).cpu().numpy()

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
    auc = roc_auc_score(y_test, y_score)

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'auc': auc,
        'epochs_run': epochs_run,
        'history': pd.DataFrame(epoch_history)
    }

def create_visualizations(history_df, experiment_name):
    """Create 3 visualizations for each experiment."""

    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    fig.suptitle(f'{experiment_name} - Training Progress', fontsize=16, fontweight='bold')

    # Plot 1: Training and Validation Loss
    axes[0].plot(history_df['epoch'], history_df['train_loss'], 'b-', label='Train Loss', linewidth=2)
    axes[0].plot(history_df['epoch'], history_df['val_loss'], 'r-', label='Val Loss', linewidth=2)
    axes[0].set_xlabel('Epoch', fontsize=12)
    axes[0].set_ylabel('Loss', fontsize=12)
    axes[0].set_title('Training vs Validation Loss', fontsize=14)
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    # Plot 2: Accuracy Comparison (Train, Val, Test)
    axes[1].plot(history_df['epoch'], history_df['train_acc'], 'b-', label='Train Acc', linewidth=2)
    axes[1].plot(history_df['epoch'], history_df['val_acc'], 'g-', label='Val Acc', linewidth=2)
    axes[1].plot(history_df['epoch'], history_df['test_acc'], 'r-', label='Test Acc', linewidth=2)
    axes[1].set_xlabel('Epoch', fontsize=12)
    axes[1].set_ylabel('Accuracy', fontsize=12)
    axes[1].set_title('Accuracy Across Sets', fontsize=14)
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)

    # Plot 3: Test AUC Over Time
    axes[2].plot(history_df['epoch'], history_df['test_auc'], 'purple', linewidth=2)
    axes[2].set_xlabel('Epoch', fontsize=12)
    axes[2].set_ylabel('AUC', fontsize=12)
    axes[2].set_title('Test AUC Over Training', fontsize=14)
    axes[2].grid(True, alpha=0.3)
    axes[2].axhline(y=history_df['test_auc'].max(), color='r', linestyle='--',
                    alpha=0.5, label=f'Max AUC: {history_df["test_auc"].max():.4f}')
    axes[2].legend()

    plt.tight_layout()
    plt.savefig(f'{experiment_name.replace(" ", "_").replace(":", "")}.png', dpi=300, bbox_inches='tight')
    plt.show()


# Run all experiments with tracking
results = []
all_histories = {}

experiments = [
    ("Control (Base Model)", {'learning_rate': 0.001, 'activation': 'relu', 'num_layers': 2, 'batch_size': 32, 'epochs': 50, 'early_stopping': False}),
    ("Q1-a: LR=0.0001 (Lower)", {'learning_rate': 0.0001, 'activation': 'relu', 'num_layers': 2, 'batch_size': 32, 'epochs': 50, 'early_stopping': False}),
    ("Q1-b: LR=0.01 (Higher)", {'learning_rate': 0.01, 'activation': 'relu', 'num_layers': 2, 'batch_size': 32, 'epochs': 50, 'early_stopping': False}),
    ("Q2: Activation=tanh", {'learning_rate': 0.001, 'activation': 'tanh', 'num_layers': 2, 'batch_size': 32, 'epochs': 50, 'early_stopping': False}),
    ("Q3: Four Layers", {'learning_rate': 0.001, 'activation': 'relu', 'num_layers': 4, 'batch_size': 32, 'epochs': 50, 'early_stopping': False}),
    ("Q4-a: Batch Size=128 (Larger)", {'learning_rate': 0.001, 'activation': 'relu', 'num_layers': 2, 'batch_size': 128, 'epochs': 50, 'early_stopping': False}),
    ("Q4-b: Batch Size=8 (Smaller)", {'learning_rate': 0.001, 'activation': 'relu', 'num_layers': 2, 'batch_size': 8, 'epochs': 50, 'early_stopping': False}),
    ("Q5: Early Stopping (Patience=5)", {'learning_rate': 0.001, 'activation': 'relu', 'num_layers': 2, 'batch_size': 32, 'epochs': 100, 'early_stopping': True})
]

for exp_name, params in experiments:
    metrics = train_and_evaluate_pytorch(**params, experiment_name=exp_name)

    results.append({
        'Experiment': exp_name,
        'Learning Rate': params['learning_rate'],
        'Activation': params['activation'],
        'Num Layers': params['num_layers'],
        'Batch Size': params['batch_size'],
        'Early Stopping': 'Yes' if params['early_stopping'] else 'No',
        'Epochs Run': metrics['epochs_run'],
        'Accuracy': metrics['accuracy'],
        'Precision': metrics['precision'],
        'Recall': metrics['recall'],
        'F1 Score': metrics['f1'],
        'AUC': metrics['auc']
    })

    all_histories[exp_name] = metrics['history']
    create_visualizations(metrics['history'], exp_name)

# Create final results table
results_df = pd.DataFrame(results)

print("\n" + "="*100)
print("PYTORCH FINAL EXPERIMENT RESULTS TABLE")
print("="*100)
print(results_df.to_string(index=False))
print("="*100)

results_df.to_csv('pytorch_results_final.csv', index=False)
print("\nResults saved to 'pytorch_results_final.csv'")

"""# TensorFlow vs PyTorch Comparison
### Detailed Interpretation of Results

## Q1 (Learning Rate): Choosing the Step Size

### Q1-a Very Low Learning Rate (0.0001) - Baby Steps
- **TensorFlow:** 87.25% accuracy, 0.880 AUC  
- **PyTorch:** 86.50% accuracy, 0.881 AUC  

**What happened:** Both frameworks made small steps and progressed very slowly. With only 50 epochs, they didn’t fully converge and performed worse than the base models.

### Q1-b Very High Learning Rate (0.01) - Giant Leaps
- **TensorFlow:** 89.00% accuracy, 0.899 AUC  
- **PyTorch:** 88.75% accuracy, 0.897 AUC  

**What happened:** High LR caused instability, but interestingly both frameworks still performed close to (or in TensorFlow’s case, slightly above) their base models this time.

**Takeaway:** The middle ground (0.001) remains clearly best for both frameworks.

## Q2: Activation Function - How Neurons Make Decisions

**Results Comparison (Tanh vs ReLU base):**
- **TensorFlow Tanh:** 87.75% accuracy, 0.904 AUC  
- **TensorFlow Base (ReLU):** 88.00% accuracy, 0.899 AUC  
- **PyTorch Tanh:** 88.75% accuracy, 0.902 AUC  
- **PyTorch Base (ReLU):** 90.25% accuracy, 0.896 AUC  

**What this means:**
**ReLU now outperforms Tanh in both frameworks** (especially strongly in PyTorch: 90.25% vs 88.75%). The difference is still small (~1–2%), confirming that for this shallow network the activation choice is not a game-changer, but ReLU is the safer default.

## Q3: Layer Depth - Simple vs Complex Networks

**Results Comparison:**
- **TensorFlow Four Layers:** 88.00% accuracy, 0.871 AUC  
- **TensorFlow Base (Two Layers):** 88.00% accuracy, 0.899 AUC  
- **PyTorch Four Layers:** **91.25% accuracy**, 0.858 AUC ( BEST overall!)  
- **PyTorch Base (Two Layers):** 90.25% accuracy, 0.896 AUC  

**Interesting finding:** The frameworks still disagree, but even more dramatically now. **PyTorch’s four-layer model is the clear winner across all experiments (91.25% accuracy)**, while adding layers hurt TensorFlow’s AUC significantly (possibly overfitting).

**Why the difference?** PyTorch’s optimizer and weight initialization appear to handle deeper networks far better on this small dataset.

## Q4: Batch Size - How Much to Learn from at Once

**Results Comparison:**
- **TensorFlow Batch=128:** 90.25% accuracy, **0.917 AUC** (excellent!)  
- **TensorFlow Batch=8:**   89.25% accuracy, 0.872 AUC  
- **PyTorch Batch=128:**   88.00% accuracy, 0.896 AUC  
- **PyTorch Batch=8:**     88.25% accuracy, 0.871 AUC  
- **Base Models (Batch=32):** TensorFlow 88.00%, PyTorch 90.25%

**Key Findings:**
1. **Larger batch (128) gave the highest AUC overall (0.917 in TensorFlow)** and very strong accuracy (90.25%)
2. Small batch (8) again performed worst in both frameworks
3. Batch=32 is no longer the clear winner — batch=128 now dominates on calibration (AUC) and is competitive on accuracy

**Conclusion:** On this dataset, **larger batches (128) provide a better speed/accuracy/AUC trade-off** than the classic batch=32 default.

## Q5: Early Stopping - Knowing When to Stop Training

**Results Comparison:**
- **TensorFlow Early Stopping:** 88.00% accuracy, 0.888 AUC (stopped at 10 epochs)  
- **TensorFlow Base:** 88.00% accuracy, 0.899 AUC (50 epochs)  
- **PyTorch Early Stopping:** 88.50% accuracy, 0.889 AUC (stopped at 13 epochs)  
- **PyTorch Base:** 90.25% accuracy, 0.896 AUC (50 epochs)

**Key Benefits:**
- Huge time savings: **~80% fewer epochs** (10–13 instead of 50)
- Very small or no accuracy drop
- Prevents overfitting — especially valuable for the deeper PyTorch model

**Conclusion:** Early stopping remains a **must-use technique** in both frameworks.

## Overall Framework Comparison Summary (Updated Nov 19, 2025)

**Similarities:**
- Both frameworks respond in the same directional way to hyperparameter changes
- Early stopping works brilliantly in both

**Differences:**
- **PyTorch achieves the highest accuracy overall (91.25% with 4 layers)**
- **TensorFlow achieves the highest AUC overall (0.917 with batch=128)**
- PyTorch scales better with added depth
- TensorFlow benefits more from larger batch sizes

**Bottom Line:**  
Both TensorFlow and PyTorch are excellent, but **PyTorch slightly edges out on raw accuracy** (91.25% best model) while **TensorFlow can achieve superior probability calibration (AUC 0.917)** with the right batch size. As always, **hyperparameter choice matters far more than the framework itself** — but these latest runs show meaningful differences in how each framework responds to depth and batch size.
"""

